{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 14, 34, 34,  8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [6,2]\n",
    "h = [1,2,5,4]\n",
    "\n",
    "# x作为卷积内核，以滑动窗口形式覆盖h，默认会将内核倒置\n",
    "y = np.convolve(x, h)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 14 34 34  8]\n",
      "[ 6 14 34 34]\n",
      "[14 34 34]\n"
     ]
    }
   ],
   "source": [
    "# “full”是默认的两侧全部填充0，“same”只在左侧填充，而“valid”则不填充。\n",
    "print(np.convolve(x,h,\"full\"))  \n",
    "print(np.convolve(x,h,\"same\"))  \n",
    "print(np.convolve(x,h,\"valid\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal as sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 其中g为卷积内核，默认会将内核旋转180°再进行卷积，\n",
    " 所以full形式中，第一行-2由[[3,2],[1,-1]]矩阵点乘[[0,0],[0,2]]矩阵再求和得到；\n",
    " 第二个-2由[[3,2],[1,-1]]矩阵点乘[[0,0],[2,4]]得到；其余类似。\n",
    " 其中“full”为全部填充0，“same”为左侧和上册填充0，而“valid”则全不填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full type\n",
      "[[-2 -2  4]\n",
      " [-7  3 34]\n",
      " [22 77 66]]\n",
      "same type\n",
      "[[-2 -2]\n",
      " [-7  3]]\n",
      "valid type\n",
      "[[3]]\n"
     ]
    }
   ],
   "source": [
    "I = [[2,4], \n",
    "    [11,22]]\n",
    "g = [[-1, 1],\n",
    "    [2, 3]]\n",
    "# 其中g为卷积内核，默认会将内核旋转180°再进行卷积，\n",
    "# 所以full形式中，第一行-2由[[3,2],[1,-1]]矩阵点乘[[0,0],[0,2]]矩阵再求和得到；\n",
    "# 第二个-2由[[3,2],[1,-1]]矩阵点乘[[0,0],[2,4]]得到；其余类似。\n",
    "# 其中“full”为全部填充0，“same”为左侧和上册填充0，而“valid”则全不填充。\n",
    "print('full type')\n",
    "print(sg.convolve(I, g, \"full\"))  \n",
    "print('same type')\n",
    "print(sg.convolve(I, g, \"same\"))  \n",
    "print('valid type')\n",
    "print(sg.convolve(I, g, \"valid\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hornsey/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图层和内核数值都由随机数产生，而随机数后面的参数设置了其形状，具体如下：\n",
    "\n",
    "input后面[1,3,3,1]表示[批量大小,宽度,高度,通道数目]\n",
    "\n",
    "filter后面[2,2,1,1]表示[宽度,高度,通道,滤波器数目]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.Variable(tf.random_normal([1,3,3,1]))\n",
    "filter = tf.Variable(tf.random_normal([2,2,1,1]))\n",
    "op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input \n",
      "\n",
      "[[[[-0.7677195 ]\n",
      "   [-0.32693413]\n",
      "   [-0.54436994]]\n",
      "\n",
      "  [[-0.23516177]\n",
      "   [ 0.5091488 ]\n",
      "   [ 0.5215869 ]]\n",
      "\n",
      "  [[-1.1619495 ]\n",
      "   [ 0.54857546]\n",
      "   [ 0.8190961 ]]]] \n",
      "\n",
      "Filter Kernel \n",
      "\n",
      "[[[[ 0.15619908]]\n",
      "\n",
      "  [[ 1.374759  ]]]\n",
      "\n",
      "\n",
      " [[[ 0.25898153]]\n",
      "\n",
      "  [[-1.0108703 ]]]] \n",
      "\n",
      "Result Feature Map with VALID position \n",
      "\n",
      "[[[[-1.1449586 ]\n",
      "   [-1.1948409 ]]\n",
      "\n",
      "  [[-0.19223735]\n",
      "   [ 0.1106558 ]]]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    print(\"Input \\n\")\n",
    "    print(\"{} \\n\".format(input.eval()))\n",
    "    print(\"Filter Kernel \\n\")\n",
    "    print(\"{} \\n\".format(filter.eval()))\n",
    "    print(\"Result Feature Map with VALID position \\n\")\n",
    "    result = sess.run(op)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取所有数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MNIST_data/train-labels.idx1-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-07e651a99f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t10k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-07e651a99f5e>\u001b[0m in \u001b[0;36mload_mnist\u001b[0;34m(path, kind)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimages_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s-images.idx3-ubyte'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlbpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmagic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>II'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# 'I'表示一个无符号整数，大小为四个字节\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MNIST_data/train-labels.idx1-ubyte'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    " \n",
    "def load_mnist(path, kind=\"train\"):\n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    " \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        # 'I'表示一个无符号整数，大小为四个字节\n",
    "        # '>II'表示读取两个无符号整数，即8个字节\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    " \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols =  struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    " \n",
    " \n",
    "X_train, y_train = load_mnist(\"MNIST_data/\", kind=\"train\")\n",
    "X_test, y_test = load_mnist(\"MNIST_data/\", kind=\"t10k\")\n",
    " \n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    " \n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    " \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取某个数字的多张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MNIST_data/train-labels.idx1-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d4b03fc4339f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t10k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d4b03fc4339f>\u001b[0m in \u001b[0;36mload_mnist\u001b[0;34m(path, kind)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimages_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s-images.idx3-ubyte'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlbpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmagic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>II'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MNIST_data/train-labels.idx1-ubyte'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    " \n",
    "def load_mnist(path, kind=\"train\"):\n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    " \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    " \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols =  struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    " \n",
    " \n",
    "X_train, y_train = load_mnist(\"MNIST_data/\", kind=\"train\")\n",
    "X_test, y_test = load_mnist(\"MNIST_data/\", kind=\"t10k\")\n",
    " \n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    " \n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X_train[y_train == 9][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    " \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hornsey/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/hornsey/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/hornsey/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/hornsey/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "width = 28 #以像素为单位的图像宽度\n",
    "height = 28 #以像素为单位的图像高度\n",
    "flat = width * height # 一个图像中的像素数\n",
    "class_output = 10 # 问题可能分类的数量\n",
    "\n",
    "x  = tf.placeholder(tf.float32, shape=[None, flat])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, class_output])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置卷积层1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、这里过滤器内核大小是5\\*5; 输入通道为1（灰度）；我们需要32个不同的特征映射（这里，32个特征映射表示32个不同的滤波器应用在每个图像上，因此卷积层的输出将是28\\*28\\*32）.\n",
    "\n",
    "2、ReLU激励函数的作用是，将covolve1中负数出现的任何地方，用0替换。\n",
    "\n",
    "3、此处最大池的内核为2\\*2，接移动步长为2，即将内核做滑动 窗口作用于图像，且取窗口内最大值。此处达到了降维的效果，最后输出为14\\*14\\*32的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs\n",
    "convolve1= tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "#ReLU激励函数\n",
    "h_conv1 = tf.nn.relu(convolve1)\n",
    "#最大池\n",
    "conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置卷积层2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处设置完成了第二层卷积。第一到第二层，输入的是[14x14x32]的图像，过滤器是[5x5x32]的内核，我们使用尺寸[5x5x32]的64级的过滤器，即取了64个特征。所以最后通过最大池，第二层输出的是[7x7x64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #需要64输出的64个偏差\n",
    "convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME')+ b_conv2\n",
    "#ReLU激励函数\n",
    "h_conv2 = tf.nn.relu(convolve2)\n",
    "#最大池\n",
    "conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置第三层完全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_matrix = tf.reshape(conv2, [-1, 7*7*64])\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) \n",
    "fcl=tf.matmul(layer2_matrix, W_fc1) + b_fc1# need 1024 biases for 1024 outputs\n",
    "\n",
    "h_fc1 = tf.nn.relu(fcl)#ReLU激励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置读出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是网络“忘记”某些功能的阶段。在小批量的每个培训步骤中，一些设备会随机关机，以免它们与网络互动。也就是说，它的权重不能被更新，也不会影响其他网络节点的学习。这对于非常大的神经网络来防止过拟合非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1)) #1024个神经元\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10])) \n",
    "fc=tf.matmul(layer_drop, W_fc2) + b_fc2\n",
    "y_CNN= tf.nn.softmax(fc)# 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数，优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_CNN), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_CNN,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1100):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, float(train_accuracy)))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
